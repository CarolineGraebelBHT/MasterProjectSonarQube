{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07c0decb-d883-493a-b3cc-1d4a781fa878",
   "metadata": {},
   "source": [
    "# CNN for Tag Prediction with ResNet\n",
    "ResNet architecture originally was invented for time-series-prediction. It is also one of the more efficient architectures when it comes to CNNs. To investigate whether the architecture is applicable to the prediction problem, a smaller architecture is used for testing purposes, ResNet18. This architecture will be used with 1D convolutional layers to predict the new tags occurring in new analysis. To ensure optimal success, the CNN is trained with the analysis data of project hive, which provides the most data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62e6b8c4-f484-4bd9-8ad5-3d4e1af9248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "import optuna\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b32fa35b-3293-465d-a83b-49c458c85f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PROJECT_ID</th>\n",
       "      <th>SQ_ANALYSIS_DATE</th>\n",
       "      <th>CLASSES</th>\n",
       "      <th>FILES</th>\n",
       "      <th>LINES</th>\n",
       "      <th>NCLOC</th>\n",
       "      <th>PACKAGE</th>\n",
       "      <th>STATEMENTS</th>\n",
       "      <th>FUNCTIONS</th>\n",
       "      <th>COMMENT_LINES</th>\n",
       "      <th>...</th>\n",
       "      <th>FUNCTION_COMPLEXITY</th>\n",
       "      <th>COGNITIVE_COMPLEXITY</th>\n",
       "      <th>LINES_TO_COVER</th>\n",
       "      <th>UNCOVERED_LINES</th>\n",
       "      <th>DUPLICATED_LINES</th>\n",
       "      <th>DUPLICATED_BLOCKS</th>\n",
       "      <th>DUPLICATED_FILES</th>\n",
       "      <th>COMMENT_LINES_DENSITY</th>\n",
       "      <th>DUPLICATED_LINES_DENSITY</th>\n",
       "      <th>TAGS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15553</th>\n",
       "      <td>hive</td>\n",
       "      <td>2008-09-02 23:58:59</td>\n",
       "      <td>613.0</td>\n",
       "      <td>358.0</td>\n",
       "      <td>67469.0</td>\n",
       "      <td>48651.0</td>\n",
       "      <td>29</td>\n",
       "      <td>26933.0</td>\n",
       "      <td>4334.0</td>\n",
       "      <td>2958.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.6</td>\n",
       "      <td>10623.0</td>\n",
       "      <td>31250.0</td>\n",
       "      <td>31250.0</td>\n",
       "      <td>16728</td>\n",
       "      <td>1204</td>\n",
       "      <td>66</td>\n",
       "      <td>5.7</td>\n",
       "      <td>24.8</td>\n",
       "      <td>error-handling, clumsy, brain-overload, design...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15552</th>\n",
       "      <td>hive</td>\n",
       "      <td>2008-09-17 00:28:22</td>\n",
       "      <td>613.0</td>\n",
       "      <td>358.0</td>\n",
       "      <td>67754.0</td>\n",
       "      <td>48873.0</td>\n",
       "      <td>29</td>\n",
       "      <td>27078.0</td>\n",
       "      <td>4340.0</td>\n",
       "      <td>2983.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.6</td>\n",
       "      <td>10691.0</td>\n",
       "      <td>31428.0</td>\n",
       "      <td>31428.0</td>\n",
       "      <td>16790</td>\n",
       "      <td>1208</td>\n",
       "      <td>66</td>\n",
       "      <td>5.8</td>\n",
       "      <td>24.8</td>\n",
       "      <td>brain-overload, clumsy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15551</th>\n",
       "      <td>hive</td>\n",
       "      <td>2008-09-17 20:13:00</td>\n",
       "      <td>613.0</td>\n",
       "      <td>358.0</td>\n",
       "      <td>67865.0</td>\n",
       "      <td>48976.0</td>\n",
       "      <td>29</td>\n",
       "      <td>27145.0</td>\n",
       "      <td>4346.0</td>\n",
       "      <td>2985.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.6</td>\n",
       "      <td>10701.0</td>\n",
       "      <td>31505.0</td>\n",
       "      <td>31505.0</td>\n",
       "      <td>16785</td>\n",
       "      <td>1208</td>\n",
       "      <td>66</td>\n",
       "      <td>5.7</td>\n",
       "      <td>24.7</td>\n",
       "      <td>convention, design</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15550</th>\n",
       "      <td>hive</td>\n",
       "      <td>2008-09-18 00:09:17</td>\n",
       "      <td>661.0</td>\n",
       "      <td>397.0</td>\n",
       "      <td>71629.0</td>\n",
       "      <td>51241.0</td>\n",
       "      <td>33</td>\n",
       "      <td>28335.0</td>\n",
       "      <td>4538.0</td>\n",
       "      <td>3215.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.6</td>\n",
       "      <td>11061.0</td>\n",
       "      <td>32889.0</td>\n",
       "      <td>32889.0</td>\n",
       "      <td>17789</td>\n",
       "      <td>1228</td>\n",
       "      <td>74</td>\n",
       "      <td>5.9</td>\n",
       "      <td>24.8</td>\n",
       "      <td>error-handling, clumsy, brain-overload, design...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15549</th>\n",
       "      <td>hive</td>\n",
       "      <td>2008-09-18 17:37:59</td>\n",
       "      <td>664.0</td>\n",
       "      <td>399.0</td>\n",
       "      <td>72263.0</td>\n",
       "      <td>51707.0</td>\n",
       "      <td>33</td>\n",
       "      <td>28559.0</td>\n",
       "      <td>4592.0</td>\n",
       "      <td>3235.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.6</td>\n",
       "      <td>11206.0</td>\n",
       "      <td>33041.0</td>\n",
       "      <td>33041.0</td>\n",
       "      <td>17659</td>\n",
       "      <td>1224</td>\n",
       "      <td>75</td>\n",
       "      <td>5.9</td>\n",
       "      <td>24.4</td>\n",
       "      <td>error-handling, clumsy, brain-overload, bad-pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13702</th>\n",
       "      <td>hive</td>\n",
       "      <td>2015-02-27 21:09:45</td>\n",
       "      <td>8327.0</td>\n",
       "      <td>3789.0</td>\n",
       "      <td>1071783.0</td>\n",
       "      <td>731599.0</td>\n",
       "      <td>364</td>\n",
       "      <td>352969.0</td>\n",
       "      <td>61412.0</td>\n",
       "      <td>75080.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.3</td>\n",
       "      <td>119218.0</td>\n",
       "      <td>431125.0</td>\n",
       "      <td>431125.0</td>\n",
       "      <td>139347</td>\n",
       "      <td>7774</td>\n",
       "      <td>791</td>\n",
       "      <td>9.3</td>\n",
       "      <td>13.0</td>\n",
       "      <td>error-handling, clumsy, design, suspicious, pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13701</th>\n",
       "      <td>hive</td>\n",
       "      <td>2015-02-27 21:30:05</td>\n",
       "      <td>8327.0</td>\n",
       "      <td>3789.0</td>\n",
       "      <td>1071783.0</td>\n",
       "      <td>731599.0</td>\n",
       "      <td>364</td>\n",
       "      <td>352969.0</td>\n",
       "      <td>61412.0</td>\n",
       "      <td>75080.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.3</td>\n",
       "      <td>119218.0</td>\n",
       "      <td>431125.0</td>\n",
       "      <td>431125.0</td>\n",
       "      <td>139347</td>\n",
       "      <td>7774</td>\n",
       "      <td>791</td>\n",
       "      <td>9.3</td>\n",
       "      <td>13.0</td>\n",
       "      <td>pitfall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13700</th>\n",
       "      <td>hive</td>\n",
       "      <td>2015-02-27 23:08:33</td>\n",
       "      <td>8468.0</td>\n",
       "      <td>3872.0</td>\n",
       "      <td>1087272.0</td>\n",
       "      <td>742901.0</td>\n",
       "      <td>387</td>\n",
       "      <td>357917.0</td>\n",
       "      <td>62390.0</td>\n",
       "      <td>76071.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.3</td>\n",
       "      <td>120954.0</td>\n",
       "      <td>437096.0</td>\n",
       "      <td>437096.0</td>\n",
       "      <td>140709</td>\n",
       "      <td>7913</td>\n",
       "      <td>810</td>\n",
       "      <td>9.3</td>\n",
       "      <td>12.9</td>\n",
       "      <td>convention, pitfall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13699</th>\n",
       "      <td>hive</td>\n",
       "      <td>2015-03-02 18:18:35</td>\n",
       "      <td>8477.0</td>\n",
       "      <td>3882.0</td>\n",
       "      <td>1088466.0</td>\n",
       "      <td>743721.0</td>\n",
       "      <td>387</td>\n",
       "      <td>358306.0</td>\n",
       "      <td>62458.0</td>\n",
       "      <td>76112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.3</td>\n",
       "      <td>121067.0</td>\n",
       "      <td>437585.0</td>\n",
       "      <td>437585.0</td>\n",
       "      <td>140806</td>\n",
       "      <td>7917</td>\n",
       "      <td>813</td>\n",
       "      <td>9.3</td>\n",
       "      <td>12.9</td>\n",
       "      <td>error-handling, design, unused, suspicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13698</th>\n",
       "      <td>hive</td>\n",
       "      <td>2015-03-03 00:37:22</td>\n",
       "      <td>8477.0</td>\n",
       "      <td>3882.0</td>\n",
       "      <td>1088490.0</td>\n",
       "      <td>743742.0</td>\n",
       "      <td>387</td>\n",
       "      <td>358319.0</td>\n",
       "      <td>62459.0</td>\n",
       "      <td>76113.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.3</td>\n",
       "      <td>121074.0</td>\n",
       "      <td>437602.0</td>\n",
       "      <td>437602.0</td>\n",
       "      <td>140806</td>\n",
       "      <td>7917</td>\n",
       "      <td>813</td>\n",
       "      <td>9.3</td>\n",
       "      <td>12.9</td>\n",
       "      <td>brain-overload, unused, antipattern, pitfall</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1856 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      PROJECT_ID    SQ_ANALYSIS_DATE  CLASSES   FILES      LINES     NCLOC  \\\n",
       "15553       hive 2008-09-02 23:58:59    613.0   358.0    67469.0   48651.0   \n",
       "15552       hive 2008-09-17 00:28:22    613.0   358.0    67754.0   48873.0   \n",
       "15551       hive 2008-09-17 20:13:00    613.0   358.0    67865.0   48976.0   \n",
       "15550       hive 2008-09-18 00:09:17    661.0   397.0    71629.0   51241.0   \n",
       "15549       hive 2008-09-18 17:37:59    664.0   399.0    72263.0   51707.0   \n",
       "...          ...                 ...      ...     ...        ...       ...   \n",
       "13702       hive 2015-02-27 21:09:45   8327.0  3789.0  1071783.0  731599.0   \n",
       "13701       hive 2015-02-27 21:30:05   8327.0  3789.0  1071783.0  731599.0   \n",
       "13700       hive 2015-02-27 23:08:33   8468.0  3872.0  1087272.0  742901.0   \n",
       "13699       hive 2015-03-02 18:18:35   8477.0  3882.0  1088466.0  743721.0   \n",
       "13698       hive 2015-03-03 00:37:22   8477.0  3882.0  1088490.0  743742.0   \n",
       "\n",
       "       PACKAGE  STATEMENTS  FUNCTIONS  COMMENT_LINES  ...  \\\n",
       "15553       29     26933.0     4334.0         2958.0  ...   \n",
       "15552       29     27078.0     4340.0         2983.0  ...   \n",
       "15551       29     27145.0     4346.0         2985.0  ...   \n",
       "15550       33     28335.0     4538.0         3215.0  ...   \n",
       "15549       33     28559.0     4592.0         3235.0  ...   \n",
       "...        ...         ...        ...            ...  ...   \n",
       "13702      364    352969.0    61412.0        75080.0  ...   \n",
       "13701      364    352969.0    61412.0        75080.0  ...   \n",
       "13700      387    357917.0    62390.0        76071.0  ...   \n",
       "13699      387    358306.0    62458.0        76112.0  ...   \n",
       "13698      387    358319.0    62459.0        76113.0  ...   \n",
       "\n",
       "       FUNCTION_COMPLEXITY  COGNITIVE_COMPLEXITY  LINES_TO_COVER  \\\n",
       "15553                  2.6               10623.0         31250.0   \n",
       "15552                  2.6               10691.0         31428.0   \n",
       "15551                  2.6               10701.0         31505.0   \n",
       "15550                  2.6               11061.0         32889.0   \n",
       "15549                  2.6               11206.0         33041.0   \n",
       "...                    ...                   ...             ...   \n",
       "13702                  2.3              119218.0        431125.0   \n",
       "13701                  2.3              119218.0        431125.0   \n",
       "13700                  2.3              120954.0        437096.0   \n",
       "13699                  2.3              121067.0        437585.0   \n",
       "13698                  2.3              121074.0        437602.0   \n",
       "\n",
       "       UNCOVERED_LINES  DUPLICATED_LINES  DUPLICATED_BLOCKS  DUPLICATED_FILES  \\\n",
       "15553          31250.0             16728               1204                66   \n",
       "15552          31428.0             16790               1208                66   \n",
       "15551          31505.0             16785               1208                66   \n",
       "15550          32889.0             17789               1228                74   \n",
       "15549          33041.0             17659               1224                75   \n",
       "...                ...               ...                ...               ...   \n",
       "13702         431125.0            139347               7774               791   \n",
       "13701         431125.0            139347               7774               791   \n",
       "13700         437096.0            140709               7913               810   \n",
       "13699         437585.0            140806               7917               813   \n",
       "13698         437602.0            140806               7917               813   \n",
       "\n",
       "       COMMENT_LINES_DENSITY  DUPLICATED_LINES_DENSITY  \\\n",
       "15553                    5.7                      24.8   \n",
       "15552                    5.8                      24.8   \n",
       "15551                    5.7                      24.7   \n",
       "15550                    5.9                      24.8   \n",
       "15549                    5.9                      24.4   \n",
       "...                      ...                       ...   \n",
       "13702                    9.3                      13.0   \n",
       "13701                    9.3                      13.0   \n",
       "13700                    9.3                      12.9   \n",
       "13699                    9.3                      12.9   \n",
       "13698                    9.3                      12.9   \n",
       "\n",
       "                                                    TAGS  \n",
       "15553  error-handling, clumsy, brain-overload, design...  \n",
       "15552                             brain-overload, clumsy  \n",
       "15551                                 convention, design  \n",
       "15550  error-handling, clumsy, brain-overload, design...  \n",
       "15549  error-handling, clumsy, brain-overload, bad-pr...  \n",
       "...                                                  ...  \n",
       "13702  error-handling, clumsy, design, suspicious, pi...  \n",
       "13701                                            pitfall  \n",
       "13700                                convention, pitfall  \n",
       "13699         error-handling, design, unused, suspicious  \n",
       "13698       brain-overload, unused, antipattern, pitfall  \n",
       "\n",
       "[1856 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data import\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# construct path to the project data folder\n",
    "data_dir = os.path.join(current_dir, '..', '..', '..', 'Data','Sonar_Issues')\n",
    "\n",
    "# load SonarQube measure data\n",
    "df = pd.read_csv(os.path.join(data_dir, 'measures+tags.csv'), low_memory=False)\n",
    "df = df[df['PROJECT_ID'] == 'hive']\n",
    "df['SQ_ANALYSIS_DATE'] = pd.to_datetime(df['SQ_ANALYSIS_DATE'])\n",
    "\n",
    "# sort the df so that the dates are ordered from oldest to newest analysis\n",
    "df = df.sort_values(by='SQ_ANALYSIS_DATE')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82057e0b-e4e8-4b8d-9dd7-389ac987d389",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fcd593-4cd0-464e-9b33-4755bd55448b",
   "metadata": {},
   "source": [
    "### Prepare labels\n",
    "The tags are given as a list of unique tags per analysis. However, to make it assessible for a CNN, the categories need to be one-hot-encoded. <br>\n",
    "For this, all tags present in the data need to be defined. Afterwards, the multi-level-binarizer is used to transform the list of unique tags for each observation into a one-hot-encoded array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa0c4344-f6b3-466d-a3c5-3848f6136202",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = ['convention', 'brain-overload','unused','error-handling','bad-practice','pitfall',\n",
    "            'clumsy','suspicious','design','antipattern','redundant','confusing','performance','obsolete']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aec0e768-c21a-4f79-afc1-fd503093f116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15553    [error-handling, clumsy, brain-overload, desig...\n",
       "15552                             [brain-overload, clumsy]\n",
       "15551                                 [convention, design]\n",
       "15550    [error-handling, clumsy, brain-overload, desig...\n",
       "15549    [error-handling, clumsy, brain-overload, bad-p...\n",
       "                               ...                        \n",
       "13702    [error-handling, clumsy, design, suspicious, p...\n",
       "13701                                            [pitfall]\n",
       "13700                                [convention, pitfall]\n",
       "13699         [error-handling, design, unused, suspicious]\n",
       "13698       [brain-overload, unused, antipattern, pitfall]\n",
       "Name: TAGS, Length: 1856, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform TAGS strings to lists\n",
    "df.loc[:, 'TAGS'] = df['TAGS'].str.split(',')\n",
    "# remove whitespaces\n",
    "df.loc[:, 'TAGS'] = df['TAGS'].apply(lambda x: [item.strip() for item in x])\n",
    "\n",
    "# save TAGS as raw_labels to be further processed\n",
    "raw_labels = df['TAGS']\n",
    "raw_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17aefa71-13df-448d-9358-38e9e0439a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLB classes (order of one-hot columns): ['convention' 'brain-overload' 'unused' 'error-handling' 'bad-practice'\n",
      " 'pitfall' 'clumsy' 'suspicious' 'design' 'antipattern' 'redundant'\n",
      " 'confusing' 'performance' 'obsolete']\n",
      "Total number of possible labels: 14\n"
     ]
    }
   ],
   "source": [
    "# initialise mlb with all tag categories\n",
    "mlb = MultiLabelBinarizer(classes=all_tags)\n",
    "# fit the mlb with the list of lists of raw labels\n",
    "mlb.fit(raw_labels)\n",
    "\n",
    "print(f\"MLB classes (order of one-hot columns): {mlb.classes_}\")\n",
    "num_classes = len(mlb.classes_)\n",
    "print(f\"Total number of possible labels: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc16d2af-0f3f-42c2-b187-70764554aaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "One-hot encoded labels (NumPy array):\n",
      "[[1 1 1 ... 1 1 1]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 1 1 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# encode raw labels with one-hot method\n",
    "one_hot_encoded_labels_array = mlb.transform(raw_labels)\n",
    "print(f\"\\nOne-hot encoded labels (NumPy array):\\n{one_hot_encoded_labels_array}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec127878-276b-476f-bf64-0dc6ef7ca395",
   "metadata": {},
   "source": [
    "### Split data into timewindows\n",
    "To accomodate the prediction task, the predictors and according labels need to be split into timewindows, to set up a shift window attention approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31cb16c1-2b92-4fbd-a27c-4bee6019b744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLASSES</th>\n",
       "      <th>FILES</th>\n",
       "      <th>LINES</th>\n",
       "      <th>NCLOC</th>\n",
       "      <th>PACKAGE</th>\n",
       "      <th>STATEMENTS</th>\n",
       "      <th>FUNCTIONS</th>\n",
       "      <th>COMMENT_LINES</th>\n",
       "      <th>COMPLEXITY</th>\n",
       "      <th>CLASS_COMPLEXITY</th>\n",
       "      <th>FUNCTION_COMPLEXITY</th>\n",
       "      <th>COGNITIVE_COMPLEXITY</th>\n",
       "      <th>LINES_TO_COVER</th>\n",
       "      <th>UNCOVERED_LINES</th>\n",
       "      <th>DUPLICATED_LINES</th>\n",
       "      <th>DUPLICATED_BLOCKS</th>\n",
       "      <th>DUPLICATED_FILES</th>\n",
       "      <th>COMMENT_LINES_DENSITY</th>\n",
       "      <th>DUPLICATED_LINES_DENSITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15553</th>\n",
       "      <td>-1.611448</td>\n",
       "      <td>-1.753882</td>\n",
       "      <td>-1.636504</td>\n",
       "      <td>-1.644758</td>\n",
       "      <td>-1.867228</td>\n",
       "      <td>-1.632894</td>\n",
       "      <td>-1.687940</td>\n",
       "      <td>-1.955492</td>\n",
       "      <td>-1.686843</td>\n",
       "      <td>0.028857</td>\n",
       "      <td>3.890931</td>\n",
       "      <td>-1.700907</td>\n",
       "      <td>-1.642291</td>\n",
       "      <td>-1.642291</td>\n",
       "      <td>-1.530234</td>\n",
       "      <td>-1.369866</td>\n",
       "      <td>-1.494250</td>\n",
       "      <td>-2.799625</td>\n",
       "      <td>2.711566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15552</th>\n",
       "      <td>-1.611448</td>\n",
       "      <td>-1.753882</td>\n",
       "      <td>-1.635665</td>\n",
       "      <td>-1.643786</td>\n",
       "      <td>-1.867228</td>\n",
       "      <td>-1.631576</td>\n",
       "      <td>-1.687632</td>\n",
       "      <td>-1.954361</td>\n",
       "      <td>-1.685957</td>\n",
       "      <td>0.028857</td>\n",
       "      <td>3.890931</td>\n",
       "      <td>-1.699043</td>\n",
       "      <td>-1.640975</td>\n",
       "      <td>-1.640975</td>\n",
       "      <td>-1.529098</td>\n",
       "      <td>-1.368268</td>\n",
       "      <td>-1.494250</td>\n",
       "      <td>-2.740282</td>\n",
       "      <td>2.711566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15551</th>\n",
       "      <td>-1.611448</td>\n",
       "      <td>-1.753882</td>\n",
       "      <td>-1.635338</td>\n",
       "      <td>-1.643336</td>\n",
       "      <td>-1.867228</td>\n",
       "      <td>-1.630968</td>\n",
       "      <td>-1.687323</td>\n",
       "      <td>-1.954270</td>\n",
       "      <td>-1.685602</td>\n",
       "      <td>0.094013</td>\n",
       "      <td>3.890931</td>\n",
       "      <td>-1.698769</td>\n",
       "      <td>-1.640406</td>\n",
       "      <td>-1.640406</td>\n",
       "      <td>-1.529189</td>\n",
       "      <td>-1.368268</td>\n",
       "      <td>-1.494250</td>\n",
       "      <td>-2.799625</td>\n",
       "      <td>2.680384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15550</th>\n",
       "      <td>-1.593440</td>\n",
       "      <td>-1.718480</td>\n",
       "      <td>-1.624254</td>\n",
       "      <td>-1.633426</td>\n",
       "      <td>-1.831254</td>\n",
       "      <td>-1.620154</td>\n",
       "      <td>-1.677453</td>\n",
       "      <td>-1.943865</td>\n",
       "      <td>-1.676188</td>\n",
       "      <td>-0.362080</td>\n",
       "      <td>3.890931</td>\n",
       "      <td>-1.688900</td>\n",
       "      <td>-1.630172</td>\n",
       "      <td>-1.630172</td>\n",
       "      <td>-1.510786</td>\n",
       "      <td>-1.360281</td>\n",
       "      <td>-1.468165</td>\n",
       "      <td>-2.680939</td>\n",
       "      <td>2.711566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15549</th>\n",
       "      <td>-1.592314</td>\n",
       "      <td>-1.716664</td>\n",
       "      <td>-1.622388</td>\n",
       "      <td>-1.631387</td>\n",
       "      <td>-1.831254</td>\n",
       "      <td>-1.618119</td>\n",
       "      <td>-1.674677</td>\n",
       "      <td>-1.942960</td>\n",
       "      <td>-1.673064</td>\n",
       "      <td>-0.296924</td>\n",
       "      <td>3.890931</td>\n",
       "      <td>-1.684926</td>\n",
       "      <td>-1.629048</td>\n",
       "      <td>-1.629048</td>\n",
       "      <td>-1.513169</td>\n",
       "      <td>-1.361878</td>\n",
       "      <td>-1.464904</td>\n",
       "      <td>-2.680939</td>\n",
       "      <td>2.586839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13702</th>\n",
       "      <td>1.282620</td>\n",
       "      <td>1.360605</td>\n",
       "      <td>1.320827</td>\n",
       "      <td>1.343395</td>\n",
       "      <td>1.145643</td>\n",
       "      <td>1.329741</td>\n",
       "      <td>1.246249</td>\n",
       "      <td>1.307311</td>\n",
       "      <td>1.277539</td>\n",
       "      <td>-0.492393</td>\n",
       "      <td>-0.656656</td>\n",
       "      <td>1.275940</td>\n",
       "      <td>1.314624</td>\n",
       "      <td>1.314624</td>\n",
       "      <td>0.717393</td>\n",
       "      <td>1.253917</td>\n",
       "      <td>0.869667</td>\n",
       "      <td>-0.663264</td>\n",
       "      <td>-0.967876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13701</th>\n",
       "      <td>1.282620</td>\n",
       "      <td>1.360605</td>\n",
       "      <td>1.320827</td>\n",
       "      <td>1.343395</td>\n",
       "      <td>1.145643</td>\n",
       "      <td>1.329741</td>\n",
       "      <td>1.246249</td>\n",
       "      <td>1.307311</td>\n",
       "      <td>1.277539</td>\n",
       "      <td>-0.492393</td>\n",
       "      <td>-0.656656</td>\n",
       "      <td>1.275940</td>\n",
       "      <td>1.314624</td>\n",
       "      <td>1.314624</td>\n",
       "      <td>0.717393</td>\n",
       "      <td>1.253917</td>\n",
       "      <td>0.869667</td>\n",
       "      <td>-0.663264</td>\n",
       "      <td>-0.967876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13700</th>\n",
       "      <td>1.335519</td>\n",
       "      <td>1.435948</td>\n",
       "      <td>1.366436</td>\n",
       "      <td>1.392846</td>\n",
       "      <td>1.352497</td>\n",
       "      <td>1.374703</td>\n",
       "      <td>1.296525</td>\n",
       "      <td>1.352144</td>\n",
       "      <td>1.323948</td>\n",
       "      <td>-0.557549</td>\n",
       "      <td>-0.656656</td>\n",
       "      <td>1.323528</td>\n",
       "      <td>1.358777</td>\n",
       "      <td>1.358777</td>\n",
       "      <td>0.742359</td>\n",
       "      <td>1.309428</td>\n",
       "      <td>0.931618</td>\n",
       "      <td>-0.663264</td>\n",
       "      <td>-0.999058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13699</th>\n",
       "      <td>1.338896</td>\n",
       "      <td>1.445025</td>\n",
       "      <td>1.369952</td>\n",
       "      <td>1.396434</td>\n",
       "      <td>1.352497</td>\n",
       "      <td>1.378238</td>\n",
       "      <td>1.300021</td>\n",
       "      <td>1.353999</td>\n",
       "      <td>1.326650</td>\n",
       "      <td>-0.557549</td>\n",
       "      <td>-0.656656</td>\n",
       "      <td>1.326626</td>\n",
       "      <td>1.362393</td>\n",
       "      <td>1.362393</td>\n",
       "      <td>0.744137</td>\n",
       "      <td>1.311025</td>\n",
       "      <td>0.941400</td>\n",
       "      <td>-0.663264</td>\n",
       "      <td>-0.999058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13698</th>\n",
       "      <td>1.338896</td>\n",
       "      <td>1.445025</td>\n",
       "      <td>1.370023</td>\n",
       "      <td>1.396526</td>\n",
       "      <td>1.352497</td>\n",
       "      <td>1.378356</td>\n",
       "      <td>1.300072</td>\n",
       "      <td>1.354044</td>\n",
       "      <td>1.326761</td>\n",
       "      <td>-0.557549</td>\n",
       "      <td>-0.656656</td>\n",
       "      <td>1.326817</td>\n",
       "      <td>1.362519</td>\n",
       "      <td>1.362519</td>\n",
       "      <td>0.744137</td>\n",
       "      <td>1.311025</td>\n",
       "      <td>0.941400</td>\n",
       "      <td>-0.663264</td>\n",
       "      <td>-0.999058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1856 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CLASSES     FILES     LINES     NCLOC   PACKAGE  STATEMENTS  \\\n",
       "15553 -1.611448 -1.753882 -1.636504 -1.644758 -1.867228   -1.632894   \n",
       "15552 -1.611448 -1.753882 -1.635665 -1.643786 -1.867228   -1.631576   \n",
       "15551 -1.611448 -1.753882 -1.635338 -1.643336 -1.867228   -1.630968   \n",
       "15550 -1.593440 -1.718480 -1.624254 -1.633426 -1.831254   -1.620154   \n",
       "15549 -1.592314 -1.716664 -1.622388 -1.631387 -1.831254   -1.618119   \n",
       "...         ...       ...       ...       ...       ...         ...   \n",
       "13702  1.282620  1.360605  1.320827  1.343395  1.145643    1.329741   \n",
       "13701  1.282620  1.360605  1.320827  1.343395  1.145643    1.329741   \n",
       "13700  1.335519  1.435948  1.366436  1.392846  1.352497    1.374703   \n",
       "13699  1.338896  1.445025  1.369952  1.396434  1.352497    1.378238   \n",
       "13698  1.338896  1.445025  1.370023  1.396526  1.352497    1.378356   \n",
       "\n",
       "       FUNCTIONS  COMMENT_LINES  COMPLEXITY  CLASS_COMPLEXITY  \\\n",
       "15553  -1.687940      -1.955492   -1.686843          0.028857   \n",
       "15552  -1.687632      -1.954361   -1.685957          0.028857   \n",
       "15551  -1.687323      -1.954270   -1.685602          0.094013   \n",
       "15550  -1.677453      -1.943865   -1.676188         -0.362080   \n",
       "15549  -1.674677      -1.942960   -1.673064         -0.296924   \n",
       "...          ...            ...         ...               ...   \n",
       "13702   1.246249       1.307311    1.277539         -0.492393   \n",
       "13701   1.246249       1.307311    1.277539         -0.492393   \n",
       "13700   1.296525       1.352144    1.323948         -0.557549   \n",
       "13699   1.300021       1.353999    1.326650         -0.557549   \n",
       "13698   1.300072       1.354044    1.326761         -0.557549   \n",
       "\n",
       "       FUNCTION_COMPLEXITY  COGNITIVE_COMPLEXITY  LINES_TO_COVER  \\\n",
       "15553             3.890931             -1.700907       -1.642291   \n",
       "15552             3.890931             -1.699043       -1.640975   \n",
       "15551             3.890931             -1.698769       -1.640406   \n",
       "15550             3.890931             -1.688900       -1.630172   \n",
       "15549             3.890931             -1.684926       -1.629048   \n",
       "...                    ...                   ...             ...   \n",
       "13702            -0.656656              1.275940        1.314624   \n",
       "13701            -0.656656              1.275940        1.314624   \n",
       "13700            -0.656656              1.323528        1.358777   \n",
       "13699            -0.656656              1.326626        1.362393   \n",
       "13698            -0.656656              1.326817        1.362519   \n",
       "\n",
       "       UNCOVERED_LINES  DUPLICATED_LINES  DUPLICATED_BLOCKS  DUPLICATED_FILES  \\\n",
       "15553        -1.642291         -1.530234          -1.369866         -1.494250   \n",
       "15552        -1.640975         -1.529098          -1.368268         -1.494250   \n",
       "15551        -1.640406         -1.529189          -1.368268         -1.494250   \n",
       "15550        -1.630172         -1.510786          -1.360281         -1.468165   \n",
       "15549        -1.629048         -1.513169          -1.361878         -1.464904   \n",
       "...                ...               ...                ...               ...   \n",
       "13702         1.314624          0.717393           1.253917          0.869667   \n",
       "13701         1.314624          0.717393           1.253917          0.869667   \n",
       "13700         1.358777          0.742359           1.309428          0.931618   \n",
       "13699         1.362393          0.744137           1.311025          0.941400   \n",
       "13698         1.362519          0.744137           1.311025          0.941400   \n",
       "\n",
       "       COMMENT_LINES_DENSITY  DUPLICATED_LINES_DENSITY  \n",
       "15553              -2.799625                  2.711566  \n",
       "15552              -2.740282                  2.711566  \n",
       "15551              -2.799625                  2.680384  \n",
       "15550              -2.680939                  2.711566  \n",
       "15549              -2.680939                  2.586839  \n",
       "...                      ...                       ...  \n",
       "13702              -0.663264                 -0.967876  \n",
       "13701              -0.663264                 -0.967876  \n",
       "13700              -0.663264                 -0.999058  \n",
       "13699              -0.663264                 -0.999058  \n",
       "13698              -0.663264                 -0.999058  \n",
       "\n",
       "[1856 rows x 19 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scale_predictors(df, label):\n",
    "    \"\"\"This function scales numerical predictor variables. The label remains unscaled.\"\"\"\n",
    "    columns_to_scale = [col for col in df.select_dtypes(include=['number']) if col != label]\n",
    "    scaler = StandardScaler()\n",
    "    df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
    "    return df\n",
    "\n",
    "df_scaled = scale_predictors(df.select_dtypes(include='number'), 'TAGS')\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2a74f37-0c09-4f0b-8218-548a8c5fe66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original df_scaled shape: (1856, 19)\n",
      "Original labels shape: (1856, 14)\n",
      "\n",
      "After windowing:\n",
      "  all_X_windows_np shape: (122, 30, 19) (samples, timesteps, features)\n",
      "  all_y_labels_np shape: (122, 14) (samples, num_unique_labels)\n",
      "\n",
      "Final PyTorch tensor shapes (before splitting):\n",
      "  all_X_data_tensor shape: torch.Size([122, 19, 30])\n",
      "  all_y_data_tensor shape: torch.Size([122, 14])\n",
      "  all_y_data_tensor dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "total_original_time_points = 1856\n",
    "n_features = 19\n",
    "num_unique_labels = 14\n",
    "\n",
    "print(f\"Original df_scaled shape: {df_scaled.shape}\")\n",
    "print(f\"Original labels shape: {one_hot_encoded_labels_array.shape}\")\n",
    "\n",
    "# define time window length (30 analysis)\n",
    "timesteps = 30\n",
    "# stride defines the overlap between time windows - time windows overlap for half their analysis here\n",
    "stride = timesteps // 2\n",
    "\n",
    "# generate sliding windows samples\n",
    "X_windows_list = []\n",
    "y_labels_list = []\n",
    "num_generated_samples = total_original_time_points - timesteps + 1\n",
    "\n",
    "for i in range(0, num_generated_samples, stride):\n",
    "    window_data = df_scaled.iloc[i : i + timesteps].values\n",
    "    X_windows_list.append(window_data)\n",
    "    label_for_window = one_hot_encoded_labels_array[i + timesteps - 1]\n",
    "    y_labels_list.append(label_for_window)\n",
    "\n",
    "# convert resulting lists of numpy array to single pytorch tensors by stacking all generated windows and their labels into one np array\n",
    "all_X_windows_np = np.array(X_windows_list)\n",
    "all_y_labels_np = np.array(y_labels_list)\n",
    "\n",
    "print(f\"\\nAfter windowing:\")\n",
    "print(f\"  all_X_windows_np shape: {all_X_windows_np.shape} (samples, timesteps, features)\")\n",
    "print(f\"  all_y_labels_np shape: {all_y_labels_np.shape} (samples, num_unique_labels)\")\n",
    "\n",
    "\n",
    "# convert arrays to pytorch tensors, permute X for Conv1D\n",
    "all_X_data_tensor = torch.tensor(all_X_windows_np, dtype=torch.float32).permute(0, 2, 1)\n",
    "all_y_data_tensor = torch.tensor(all_y_labels_np, dtype=torch.float32)\n",
    "\n",
    "print(f\"\\nFinal PyTorch tensor shapes (before splitting):\")\n",
    "print(f\"  all_X_data_tensor shape: {all_X_data_tensor.shape}\")\n",
    "print(f\"  all_y_data_tensor shape: {all_y_data_tensor.shape}\")\n",
    "# check if y data is torch.float32 for BCEWithLogitsLoss\n",
    "print(f\"  all_y_data_tensor dtype: {all_y_data_tensor.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fd3f26-9ee6-4132-8530-4b99f155bf24",
   "metadata": {},
   "source": [
    "### Train-Validation-Test-Split\n",
    "For this model, the data is split into 3 different sets, using training and validation set for hyperparameter optimisation. The training and testing set are used to train and evaluate the model with optimised hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "397ff04b-2dff-4b8b-b795-328b0cd547fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split dataset sizes:\n",
      "  Train set X shape: torch.Size([97, 19, 30]), y shape: torch.Size([97, 14])\n",
      "  Validation set X shape: torch.Size([12, 19, 30]), y shape: torch.Size([12, 14])\n",
      "  Test set X shape: torch.Size([13, 19, 30]), y shape: torch.Size([13, 14])\n"
     ]
    }
   ],
   "source": [
    "# split data into training, validation and testing set\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "train_split_idx = int(all_X_data_tensor.shape[0] * train_ratio)\n",
    "val_split_idx = int(all_X_data_tensor.shape[0] * (train_ratio + val_ratio))\n",
    "\n",
    "X_train_tensor = all_X_data_tensor[:train_split_idx]\n",
    "y_train_tensor = all_y_data_tensor[:train_split_idx]\n",
    "\n",
    "X_val_tensor = all_X_data_tensor[train_split_idx:val_split_idx]\n",
    "y_val_tensor = all_y_data_tensor[train_split_idx:val_split_idx]\n",
    "\n",
    "X_test_tensor = all_X_data_tensor[val_split_idx:]\n",
    "y_test_tensor = all_y_data_tensor[val_split_idx:]\n",
    "\n",
    "print(f\"\\nSplit dataset sizes:\")\n",
    "print(f\"  Train set X shape: {X_train_tensor.shape}, y shape: {y_train_tensor.shape}\")\n",
    "print(f\"  Validation set X shape: {X_val_tensor.shape}, y shape: {y_val_tensor.shape}\")\n",
    "print(f\"  Test set X shape: {X_test_tensor.shape}, y shape: {y_test_tensor.shape}\")\n",
    "\n",
    "\n",
    "# creating pytorch dataset objects for dataloader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efba415-c126-442a-83da-a693c0a8552c",
   "metadata": {},
   "source": [
    "## 1D ResNet18 architecture\n",
    "The architecture follows a ResNet18 architecture ([Diagram of original ResNet18 architecture](https://www.researchgate.net/profile/Poorya-Mohammadinasab/publication/373653509/figure/fig1/AS:11431281186311794@1693861891854/Original-ResNet-18-Architecture.png)). It is adapted to fit a 1D-problem, meaning that each predictor is passed as a 1D-vector. ResNet has the ability to \"skip connections\": this allows information to bypass some layers, helping the network learn more effectively even when it's very deep. It consists of a convolutional layer with a batch normalisation, ReLu activation and a max pool layer at the beginning. This is followed by four blocks. Each block contains two convolutional layers, again followed by batch normalization and ReLU activation. Once the data has passed through all four building block stages, an average pooling layer summarizes the learned features. This prepares the data for the final step: a linear transformation. This final layer outputs raw logits, which are essentially scores for each possible \"tag\" or category. These scores are then used to determine the probabilities that a specific tag appears in your analysis, making it suitable for multi-classification tasks where multiple tags can apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84ced041-b1e3-41ed-9013-5bae61212b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock1D(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        # this will ensure 'identity' has the same dimensions as 'out' if needed\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet1D(nn.Module):\n",
    "    def __init__(self, block, layers, in_channels, num_classes):\n",
    "        super(ResNet1D, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels, self.in_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(self.in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=1)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        # no activation function, outputs raw logits\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels * block.expansion),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "def resnet18_1d(in_channels, num_classes):\n",
    "    \"\"\"\n",
    "    ResNet-18 (1D)\n",
    "    Equivalent to original ResNet-18 for images, but with Conv1d\n",
    "    layers = [2, 2, 2, 2] means 2 BasicBlocks in each of the 4 stages\n",
    "    \"\"\"\n",
    "    return ResNet1D(BasicBlock1D, [2, 2, 2, 2], in_channels, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce269b7b-9e3f-4263-b239-7af3846cdacb",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimisation and Training\n",
    "Hyperparameter Optimisation is done using SciKit Learn's Optuna. Optuna tries promising combinations of hyperparameters over 50 different trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca6fd14f-1b73-4dab-b773-f3b8442ae222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup function for Optuna\n",
    "def objective(trial):\n",
    "    # hyperparameters to test in trial\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128])\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    \n",
    "    # initialise model\n",
    "    model = resnet18_1d(in_channels=n_features, num_classes=num_unique_labels)\n",
    "    model.to(device)\n",
    "\n",
    "    # create dataloarders for training and validation data\n",
    "    train_loader_optuna = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader_optuna = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # initialise optimisers\n",
    "    if optimizer_name == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"RMSprop\":\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # training loops for trial (5 epochs per trial)\n",
    "    N_EPOCHS_PER_TRIAL = 5\n",
    "    best_val_loss_trial = float('inf')\n",
    "\n",
    "    # run training\n",
    "    for epoch in range(N_EPOCHS_PER_TRIAL):\n",
    "        model.train()\n",
    "        for batch_idx, (data, labels) in enumerate(train_loader_optuna):\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # evaluate model performance on validation set\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, labels) in enumerate(val_loader_optuna):\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_val_loss += loss.item() * data.size(0)\n",
    "        epoch_val_loss = running_val_loss / len(val_loader_optuna.dataset)\n",
    "\n",
    "        # initialize Optuna's pruning for unpromising trials (saves runtime)\n",
    "        trial.report(epoch_val_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if epoch_val_loss < best_val_loss_trial:\n",
    "            best_val_loss_trial = epoch_val_loss\n",
    "\n",
    "    # return smallest validation loss\n",
    "    return best_val_loss_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40f61b76-9d15-4ebd-bf4d-88914be5e0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-30 17:27:49,789] A new study created in memory with name: resnet1d_hpo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Starting Hyperparameter Optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-30 17:27:56,271] Trial 0 finished with value: 0.6766470074653625 and parameters: {'lr': 0.00020652648689075718, 'batch_size': 128, 'optimizer': 'Adam'}. Best is trial 0 with value: 0.6766470074653625.\n",
      "[I 2025-05-30 17:28:08,073] Trial 1 finished with value: 7.0781683921813965 and parameters: {'lr': 6.068281157818735e-05, 'batch_size': 16, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.6766470074653625.\n",
      "[I 2025-05-30 17:28:20,458] Trial 2 finished with value: 22.388578414916992 and parameters: {'lr': 0.015375219589336199, 'batch_size': 16, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.6766470074653625.\n",
      "[I 2025-05-30 17:28:24,402] Trial 3 finished with value: 0.5280751585960388 and parameters: {'lr': 0.0028463669690439535, 'batch_size': 64, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.5280751585960388.\n",
      "[I 2025-05-30 17:28:26,987] Trial 4 finished with value: 0.6991267204284668 and parameters: {'lr': 0.0014616391228002826, 'batch_size': 128, 'optimizer': 'SGD'}. Best is trial 3 with value: 0.5280751585960388.\n",
      "[I 2025-05-30 17:28:27,754] Trial 5 pruned. \n",
      "[I 2025-05-30 17:28:30,401] Trial 6 pruned. \n",
      "[I 2025-05-30 17:28:32,837] Trial 7 pruned. \n",
      "[I 2025-05-30 17:28:34,021] Trial 8 pruned. \n",
      "[I 2025-05-30 17:28:34,761] Trial 9 pruned. \n",
      "[I 2025-05-30 17:28:39,152] Trial 10 finished with value: 0.5095024108886719 and parameters: {'lr': 0.0019648247634796324, 'batch_size': 64, 'optimizer': 'Adam'}. Best is trial 10 with value: 0.5095024108886719.\n",
      "[I 2025-05-30 17:28:45,113] Trial 11 finished with value: 0.4936797320842743 and parameters: {'lr': 0.0019004009563523354, 'batch_size': 64, 'optimizer': 'Adam'}. Best is trial 11 with value: 0.4936797320842743.\n",
      "[I 2025-05-30 17:28:46,064] Trial 12 pruned. \n",
      "[I 2025-05-30 17:28:51,276] Trial 13 finished with value: 0.6012645363807678 and parameters: {'lr': 0.0003449690493299212, 'batch_size': 64, 'optimizer': 'Adam'}. Best is trial 11 with value: 0.4936797320842743.\n",
      "[I 2025-05-30 17:28:57,139] Trial 14 finished with value: 0.6182245016098022 and parameters: {'lr': 0.004203037437412423, 'batch_size': 64, 'optimizer': 'Adam'}. Best is trial 11 with value: 0.4936797320842743.\n",
      "[I 2025-05-30 17:28:58,270] Trial 15 pruned. \n",
      "[I 2025-05-30 17:29:07,310] Trial 16 finished with value: 0.4889174997806549 and parameters: {'lr': 0.00041251065176422646, 'batch_size': 32, 'optimizer': 'Adam'}. Best is trial 16 with value: 0.4889174997806549.\n",
      "[I 2025-05-30 17:29:15,473] Trial 17 finished with value: 0.5113941431045532 and parameters: {'lr': 0.00032679313575236256, 'batch_size': 32, 'optimizer': 'Adam'}. Best is trial 16 with value: 0.4889174997806549.\n",
      "[I 2025-05-30 17:29:17,286] Trial 18 pruned. \n",
      "[I 2025-05-30 17:29:24,776] Trial 19 finished with value: 0.4909326136112213 and parameters: {'lr': 0.0006038221512993601, 'batch_size': 32, 'optimizer': 'RMSprop'}. Best is trial 16 with value: 0.4889174997806549.\n",
      "[I 2025-05-30 17:29:26,339] Trial 20 pruned. \n",
      "[I 2025-05-30 17:29:34,870] Trial 21 finished with value: 0.5018791556358337 and parameters: {'lr': 0.0006751873872301095, 'batch_size': 32, 'optimizer': 'RMSprop'}. Best is trial 16 with value: 0.4889174997806549.\n",
      "[I 2025-05-30 17:29:36,759] Trial 22 pruned. \n",
      "[I 2025-05-30 17:29:43,158] Trial 23 finished with value: 0.5141770243644714 and parameters: {'lr': 0.0006129207801157902, 'batch_size': 32, 'optimizer': 'RMSprop'}. Best is trial 16 with value: 0.4889174997806549.\n",
      "[I 2025-05-30 17:29:45,132] Trial 24 pruned. \n",
      "[I 2025-05-30 17:29:46,463] Trial 25 pruned. \n",
      "[I 2025-05-30 17:29:48,235] Trial 26 pruned. \n",
      "[I 2025-05-30 17:29:49,671] Trial 27 pruned. \n",
      "C:\\Users\\carol\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\pruners\\_percentile.py:21: RuntimeWarning: All-NaN slice encountered\n",
      "  return np.nanmin(values)\n",
      "[I 2025-05-30 17:29:51,547] Trial 28 pruned. \n",
      "[I 2025-05-30 17:29:52,368] Trial 29 pruned. \n",
      "[I 2025-05-30 17:29:53,528] Trial 30 pruned. \n",
      "[I 2025-05-30 17:30:01,201] Trial 31 finished with value: 0.5101136565208435 and parameters: {'lr': 0.0006261217502548957, 'batch_size': 32, 'optimizer': 'RMSprop'}. Best is trial 16 with value: 0.4889174997806549.\n",
      "[I 2025-05-30 17:30:02,269] Trial 32 pruned. \n",
      "[I 2025-05-30 17:30:10,614] Trial 33 finished with value: 0.4938667416572571 and parameters: {'lr': 0.00028018638282455155, 'batch_size': 32, 'optimizer': 'RMSprop'}. Best is trial 16 with value: 0.4889174997806549.\n",
      "[I 2025-05-30 17:30:18,102] Trial 34 pruned. \n",
      "[I 2025-05-30 17:30:19,481] Trial 35 pruned. \n",
      "[I 2025-05-30 17:30:21,142] Trial 36 pruned. \n",
      "[I 2025-05-30 17:30:21,891] Trial 37 pruned. \n",
      "[I 2025-05-30 17:30:25,515] Trial 38 pruned. \n",
      "[I 2025-05-30 17:30:26,021] Trial 39 pruned. \n",
      "[I 2025-05-30 17:30:27,912] Trial 40 pruned. \n",
      "[I 2025-05-30 17:30:36,211] Trial 41 finished with value: 0.48088377714157104 and parameters: {'lr': 0.0005383280210998608, 'batch_size': 32, 'optimizer': 'RMSprop'}. Best is trial 41 with value: 0.48088377714157104.\n",
      "[I 2025-05-30 17:30:43,160] Trial 42 finished with value: 0.4917528033256531 and parameters: {'lr': 0.0003858239965986135, 'batch_size': 32, 'optimizer': 'RMSprop'}. Best is trial 41 with value: 0.48088377714157104.\n",
      "[I 2025-05-30 17:30:50,186] Trial 43 finished with value: 0.5068179965019226 and parameters: {'lr': 0.0005192922404762889, 'batch_size': 32, 'optimizer': 'RMSprop'}. Best is trial 41 with value: 0.48088377714157104.\n",
      "[I 2025-05-30 17:30:52,264] Trial 44 pruned. \n",
      "[I 2025-05-30 17:30:52,972] Trial 45 pruned. \n",
      "[I 2025-05-30 17:31:00,547] Trial 46 finished with value: 0.5128996968269348 and parameters: {'lr': 0.0023586125010028633, 'batch_size': 32, 'optimizer': 'Adam'}. Best is trial 41 with value: 0.48088377714157104.\n",
      "[I 2025-05-30 17:31:01,919] Trial 47 pruned. \n",
      "[I 2025-05-30 17:31:03,991] Trial 48 pruned. \n",
      "[I 2025-05-30 17:31:05,388] Trial 49 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyperparameter Optimization Finished.\n",
      "Number of finished trials: 50\n",
      "Best trial parameters: {'lr': 0.0005383280210998608, 'batch_size': 32, 'optimizer': 'RMSprop'}\n",
      "Best trial validation loss: 0.4809\n",
      "\n",
      "Retraining the final model with best hyperparameters...\n",
      "Calculated pos_weight for each class: tensor([ 1.0638,  1.6216,  2.4643,  1.6216,  2.2333,  3.2174,  3.4091,  2.5926,\n",
      "         3.2174,  4.7059,  9.7778, 47.5000, 15.1667, 95.9999])\n",
      "Final Training - Epoch 1/50, Train Loss: 1.5180, Val Loss: 1.5510\n",
      "  Final model val loss improved. Saving to C:\\Users\\carol\\Dropbox\\DataScience\\Semester4\\MasterProjectSonarQube\\Scripts\\Model\\tagPrediction\\..\\..\\..\\Data\\Models\\CodeSmellTags\\ResNet_1D\\optimised_best_resnet_cnn.pth. Best Loss: 1.5510\n",
      "Final Training - Epoch 2/50, Train Loss: 1.3668, Val Loss: 1.5872\n",
      "Final Training - Epoch 3/50, Train Loss: 1.1896, Val Loss: 1.7864\n",
      "Final Training - Epoch 4/50, Train Loss: 1.1545, Val Loss: 1.9187\n",
      "Final Training - Epoch 5/50, Train Loss: 1.0074, Val Loss: 2.2691\n",
      "Final Training - Epoch 6/50, Train Loss: 0.9762, Val Loss: 2.4478\n",
      "Final Training - Epoch 7/50, Train Loss: 1.0409, Val Loss: 2.6439\n",
      "Final Training - Epoch 8/50, Train Loss: 0.9475, Val Loss: 3.2368\n",
      "Final Training - Epoch 9/50, Train Loss: 0.9124, Val Loss: 3.7609\n",
      "Final Training - Epoch 10/50, Train Loss: 0.8807, Val Loss: 3.5351\n",
      "Final Training - Epoch 11/50, Train Loss: 0.8940, Val Loss: 3.4855\n",
      "  Final training early stopping triggered after 11 epochs.\n",
      "\n",
      "Final model training complete.\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter optimisation function\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # create Optuna study that minimises validation loss\n",
    "    study = optuna.create_study(direction=\"minimize\", study_name=\"resnet1d_hpo\")\n",
    "\n",
    "    # run optimisation with 50 trials\n",
    "    print(\"\\nStarting Hyperparameter Optimization...\")\n",
    "    study.optimize(objective, n_trials=50)\n",
    "\n",
    "    print(\"\\nHyperparameter Optimization Finished.\")\n",
    "    print(f\"Number of finished trials: {len(study.trials)}\")\n",
    "    print(f\"Best trial parameters: {study.best_trial.params}\")\n",
    "    print(f\"Best trial validation loss: {study.best_trial.value:.4f}\")\n",
    "\n",
    "    # extract parameters for the trial with the lowest validation loss\n",
    "    print(\"\\nRetraining the final model with best hyperparameters...\")\n",
    "    best_params = study.best_trial.params\n",
    "\n",
    "    final_lr = best_params[\"lr\"]\n",
    "    final_batch_size = best_params[\"batch_size\"]\n",
    "    final_optimizer_name = best_params[\"optimizer\"]\n",
    "\n",
    "    final_model = resnet18_1d(in_channels=n_features, num_classes=num_unique_labels)\n",
    "    final_model.to(device)\n",
    "\n",
    "    # load test set as well as the others for the final model with optimised hyperparameters\n",
    "    final_train_loader = DataLoader(train_dataset, batch_size=final_batch_size, shuffle=True)\n",
    "    final_val_loader = DataLoader(val_dataset, batch_size=final_batch_size, shuffle=False)\n",
    "    final_test_loader = DataLoader(test_dataset, batch_size=final_batch_size, shuffle=False)\n",
    "\n",
    "    # weighting\n",
    "    training_labels = y_train_tensor.cpu().numpy()\n",
    "\n",
    "    num_classes = training_labels.shape[1]\n",
    "    positive_counts = np.sum(training_labels == 1, axis=0)\n",
    "    negative_counts = np.sum(training_labels == 0, axis=0)\n",
    "\n",
    "    # calculate pos_weight for each class\n",
    "    pos_weight = torch.zeros(num_classes)\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        if positive_counts[i] == 0:\n",
    "            print(f\"Warning: Class {i} has no positive samples in the training data.\")\n",
    "            pos_weight[i] = torch.tensor(1.0)\n",
    "        else:\n",
    "            pos_weight[i] = torch.tensor(negative_counts[i] / (positive_counts[i] + epsilon))\n",
    "\n",
    "    # add pos_weight to device\n",
    "    pos_weight = pos_weight.to(device)\n",
    "    \n",
    "    print(f\"Calculated pos_weight for each class: {pos_weight}\")\n",
    "    \n",
    "    # intiating Loss Function with pos_weight\n",
    "    final_criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    if final_optimizer_name == \"Adam\":\n",
    "        final_optimizer = optim.Adam(final_model.parameters(), lr=final_lr)\n",
    "    elif final_optimizer_name == \"RMSprop\":\n",
    "        final_optimizer = optim.RMSprop(final_model.parameters(), lr=final_lr)\n",
    "    else: # SGD\n",
    "        final_optimizer = optim.SGD(final_model.parameters(), lr=final_lr, momentum=0.9)\n",
    "\n",
    "    # train cnn with optimised hyperparameters\n",
    "    FINAL_NUM_EPOCHS = 50\n",
    "    final_best_val_loss = float('inf')\n",
    "    final_epochs_no_improve = 0\n",
    "    # for early-stopping (after 10 epochs of no improvement)\n",
    "    final_patience = 10\n",
    "\n",
    "    # model path for saving the best model\n",
    "    model_save_dir = os.path.join(current_dir, '..', '..', '..', 'Data', 'Models', 'CodeSmellTags', 'ResNet_1D')\n",
    "    final_model_save_path = os.path.join(model_save_dir, 'optimised_best_resnet_cnn.pth')\n",
    "\n",
    "    # run training with optimised hyperparameters\n",
    "    for epoch in range(FINAL_NUM_EPOCHS):\n",
    "        final_model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for batch_idx, (data, labels) in enumerate(final_train_loader):\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            final_optimizer.zero_grad()\n",
    "            outputs = final_model(data)\n",
    "            loss = final_criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            final_optimizer.step()\n",
    "            running_train_loss += loss.item() * data.size(0)\n",
    "        epoch_train_loss = running_train_loss / len(final_train_loader.dataset)\n",
    "\n",
    "        final_model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, labels) in enumerate(final_val_loader):\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                outputs = final_model(data)\n",
    "                loss = final_criterion(outputs, labels)\n",
    "                running_val_loss += loss.item() * data.size(0)\n",
    "        epoch_val_loss = running_val_loss / len(final_val_loader.dataset)\n",
    "\n",
    "        print(f\"Final Training - Epoch {epoch+1}/{FINAL_NUM_EPOCHS}, Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}\")\n",
    "\n",
    "        # save model if validation loss decreases\n",
    "        if epoch_val_loss < final_best_val_loss:\n",
    "            final_best_val_loss = epoch_val_loss\n",
    "            final_epochs_no_improve = 0\n",
    "            torch.save(final_model.state_dict(), final_model_save_path)\n",
    "            print(f\"  Final model val loss improved. Saving to {final_model_save_path}. Best Loss: {final_best_val_loss:.4f}\")\n",
    "        else:\n",
    "            final_epochs_no_improve += 1\n",
    "            if final_epochs_no_improve >= final_patience:\n",
    "                print(f\"  Final training early stopping triggered after {epoch+1} epochs.\")\n",
    "                break\n",
    "\n",
    "    print(\"\\nFinal model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61df5b9-2a8a-41f8-b1fd-8da913e689d3",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d1d52f2-313f-486e-a07b-e03058713ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.8045\n",
      "\n",
      "Detailed Test Set Metrics:\n",
      "  Micro F1-score: 0.3099\n",
      "  Micro Precision: 0.2821\n",
      "  Micro Recall: 0.3438\n",
      "  Macro F1-score: 0.0836\n",
      "  Macro Precision: 0.0604\n",
      "  Macro Recall: 0.1429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carol\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "C:\\Users\\carol\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\carol\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# evaluate model performance on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "# load the best model saved\n",
    "final_model.load_state_dict(torch.load(final_model_save_path))\n",
    "final_model.to(device)\n",
    "final_model.eval()\n",
    "\n",
    "test_loss = 0.0\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, labels) in enumerate(final_test_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        outputs = final_model(data)\n",
    "        loss = final_criterion(outputs, labels)\n",
    "        test_loss += loss.item() * data.size(0)\n",
    "        probabilities = torch.sigmoid(outputs)\n",
    "        all_preds.append(probabilities.cpu().numpy())\n",
    "        all_targets.append(labels.cpu().numpy())\n",
    "\n",
    "avg_test_loss = test_loss / len(final_test_loader.dataset)\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "all_preds = np.vstack(all_preds)\n",
    "all_targets = np.vstack(all_targets)\n",
    "\n",
    "# calculate detailed metrics\n",
    "threshold = 0.5\n",
    "binary_preds = (all_preds > threshold).astype(int)\n",
    "\n",
    "print(\"\\nDetailed Test Set Metrics:\")\n",
    "print(f\"  Micro F1-score: {f1_score(all_targets, binary_preds, average='micro'):.4f}\")\n",
    "print(f\"  Micro Precision: {precision_score(all_targets, binary_preds, average='micro'):.4f}\")\n",
    "print(f\"  Micro Recall: {recall_score(all_targets, binary_preds, average='micro'):.4f}\")\n",
    "print(f\"  Macro F1-score: {f1_score(all_targets, binary_preds, average='macro'):.4f}\")\n",
    "print(f\"  Macro Precision: {precision_score(all_targets, binary_preds, average='macro'):.4f}\")\n",
    "print(f\"  Macro Recall: {recall_score(all_targets, binary_preds, average='macro'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dae23ee-2d55-4d5e-9731-e62ebc72164b",
   "metadata": {},
   "source": [
    "## Result\n",
    "CNNs are not suited to model the problem, since there is too little data to properly train, validate and test a CNN. Even though the model architecture fits the issue in theory, the result isn't properly trained enough to give a strong idea about performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
